# Cursor Rules for Integrated Image Labeling & Fine-Tuning Tool

## üß™ Functional Testing Requirements

### **Mandatory Testing Before Deployment**
- ALWAYS run functional tests with headless Chrome before deploying or committing changes
- Use `python run_tests.py` to automatically test the integrated app
- Ensure all tests pass before spending time manually testing the app
- Run tests in CI/CD pipeline for every pull request

### **Testing Dependencies**
- Install testing requirements: `pip install -r requirements_testing.txt`
- Ensure Chrome browser is installed for Selenium testing
- Use webdriver-manager for automatic ChromeDriver management

### **Test Coverage Requirements**
- Test app loading and responsiveness
- Verify sidebar elements and navigation
- Test tab switching between Image Labeling and Fine-Tuning
- Validate AI model selection and configuration
- Test GCP authentication elements
- Verify Gemini integration features
- Test Vertex AI integration components
- Check app performance and interaction capabilities

### **Testing Commands**
```bash
# Quick test run (starts app automatically)
python run_tests.py

# Test with custom port
python run_tests.py --port 8503

# Test without starting app (if already running)
python run_tests.py --no-start-app

# Run with visible browser (for debugging)
python run_tests.py --no-headless

# Check dependencies only
python run_tests.py --check-only

# Direct test execution
python test_integrated_app.py --url http://localhost:8502
```

### **Test Quality Standards**
- All critical functionality must have automated tests
- Tests should be deterministic and reliable
- Use headless Chrome for CI/CD compatibility
- Capture screenshots on test failures for debugging
- Include performance testing for app responsiveness
- Test both success and failure scenarios

## üöÄ Development Workflow

### **Before Making Changes**
1. Run existing tests: `python run_tests.py --no-start-app`
2. Ensure all tests pass before making changes
3. Create feature branch for new development

### **During Development**
1. Write tests for new features before implementing them
2. Run tests frequently during development
3. Use `--no-headless` flag for debugging UI issues
4. Test with different ports to avoid conflicts

### **Before Committing**
1. Run full test suite: `python run_tests.py`
2. Ensure all tests pass
3. Add new tests for any new functionality
4. Update test documentation if needed

### **Before Deployment**
1. Run tests in production-like environment
2. Test with real GCP credentials (if available)
3. Verify all integrations work correctly
4. Check performance under load

## üîß Technical Requirements

### **Code Quality**
- Follow PEP 8 style guidelines
- Use type hints for all function parameters and return values
- Write comprehensive docstrings for all functions
- Handle exceptions gracefully with meaningful error messages
- Use environment variables for configuration

### **Testing Best Practices**
- Write isolated, independent tests
- Use descriptive test names and messages
- Implement proper setup and teardown
- Use explicit waits instead of implicit waits
- Handle flaky tests with retry mechanisms
- Test both positive and negative scenarios

### **Performance Requirements**
- App should load within 5 seconds
- Tests should complete within 2 minutes
- Handle concurrent users gracefully
- Optimize for memory usage
- Use efficient selectors for element location

### **Security Requirements**
- Never commit API keys or credentials
- Use environment variables for sensitive data
- Validate all user inputs
- Implement proper authentication checks
- Test security boundaries

## üìã Integration Requirements

### **GCP Integration**
- Test authentication with service account keys
- Verify Vertex AI API access
- Test Cloud Storage operations
- Validate model deployment workflows
- Check endpoint prediction functionality

### **AI Model Integration**
- Test YOLO model loading and inference
- Verify Transformers model functionality
- Test Gemini API integration
- Validate confidence threshold filtering
- Check batch processing capabilities

### **Streamlit Integration**
- Test all Streamlit components
- Verify session state management
- Test file upload functionality
- Validate sidebar interactions
- Check tab navigation

## üêõ Debugging Guidelines

### **When Tests Fail**
1. Run tests with `--verbose` flag for detailed output
2. Use `--no-headless` to see browser interactions
3. Check Chrome and ChromeDriver versions
4. Verify app is running and accessible
5. Review test logs for specific error messages

### **Common Issues**
- Chrome driver compatibility issues
- App not starting or accessible
- Element not found errors
- Timeout issues on slow systems
- Memory issues with large datasets

### **Debugging Commands**
```bash
# Check app availability
python test_integrated_app.py --check-only

# Run with visible browser
python run_tests.py --no-headless

# Test specific URL
python test_integrated_app.py --url http://localhost:8502

# Check dependencies
python run_tests.py --check-only
```

## üìä Quality Metrics

### **Success Criteria**
- All tests must pass (PASS status)
- No critical functionality failures
- Performance within acceptable limits
- Security requirements met
- Documentation up to date

### **Monitoring**
- Track test execution time
- Monitor test failure rates
- Measure code coverage
- Track performance metrics
- Monitor security vulnerabilities

## üîÑ Continuous Integration

### **Automated Testing**
- Run tests on every commit
- Test multiple Python versions
- Test on different operating systems
- Validate all integrations
- Check for security issues

### **Deployment Pipeline**
- Run full test suite before deployment
- Validate production environment
- Test with real data (if available)
- Monitor deployment success
- Rollback on test failures

## üìö Documentation Requirements

### **Code Documentation**
- Comprehensive docstrings for all functions
- Type hints for all parameters
- Example usage in docstrings
- Clear error messages and handling

### **Test Documentation**
- Document test scenarios and coverage
- Explain test data requirements
- Provide troubleshooting guides
- Include performance benchmarks

### **User Documentation**
- Clear setup instructions
- Usage examples and workflows
- Troubleshooting guides
- API documentation

## üéØ Success Metrics

### **Development Efficiency**
- Reduced manual testing time
- Faster bug detection
- Improved code quality
- Better user experience

### **Reliability**
- Consistent test results
- Reduced production issues
- Better error handling
- Improved performance

### **Maintainability**
- Clear test structure
- Easy to add new tests
- Well-documented code
- Modular architecture

---

**Remember: Always test before you invest time in manual testing! üß™** 